{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of this notebook is to classify review into correct categories, so this is a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from srs.utilities import Sentence,loadUsefulTrainingData,loadTrainingData,tokenize\n",
    "from srs.maxEntropyModel import loadWordListDict\n",
    "from srs.predictor import StaticPredictor\n",
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "static_traning_data_dir = os.path.abspath('../srs/static_training_data/')\n",
    "sentences = loadUsefulTrainingData(static_traning_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in sentences:\n",
    "    s.tokens = tokenize(s.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'shot', u'color', u'fit', u'cheap', u'focus', u'bulki', u'design', u'batteri', u'simpli', u'size', u'use', u'detect', u'easi', u'pictur', u'screen', u'carri', u'big', u'simpl', u'price', u'smart', u'mode', u'beatiful', u'afford', u'auto', u'imag', u'qualiti', u'len', u'cheaper', u'pocket', u'pretti', u'charger', u'clear', u'zoom', u'easili', u'expens', u'small', u'video', u'nice', u'display']\n"
     ]
    }
   ],
   "source": [
    "wordlist_dict_path = \"../srs/predictor_data/wordlist_dict_1.txt\"\n",
    "feature_dict = loadWordListDict(wordlist_dict_path)\n",
    "feature_list = []\n",
    "for k in feature_dict.keys():\n",
    "    feature_list = feature_list + feature_dict[k]\n",
    "feature_list = list(set(feature_list)) #remove duplicates if any\n",
    "print feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'battery': [u'batteri', u'charger'], u'pictures': [u'pictur', u'imag', u'shot'], u'price': [u'cheap', u'cheaper', u'expens', u'afford', u'price'], u'zoom': [u'zoom', u'len'], u'ease of use': [u'easi', u'simpl', u'easili', u'simpli', u'use'], u'detection': [u'detect', u'auto', u'mode', u'smart', u'focus'], u'design': [u'design', u'nice', u'beatiful', u'color', u'pretti'], u'video': [u'clear', u'video'], u'quality': [u'qualiti'], u'screen': [u'screen', u'display'], u'size': [u'size', u'big', u'small', u'fit', u'carri', u'pocket', u'bulki']}\n"
     ]
    }
   ],
   "source": [
    "print feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  2.\n",
      "  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "s1 = sentences[1]\n",
    "present_list = np.zeros(len(feature_list))\n",
    "for token in s1.tokens:\n",
    "    if token in feature_list:\n",
    "        idx = feature_list.index(token)\n",
    "        present_list[idx] += 1\n",
    "print present_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Panda framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=feature_list)\n",
    "target = pd.DataFrame(columns=['Prod_Feat'])\n",
    "for s in sentences:\n",
    "    count_list = np.zeros(len(feature_list))\n",
    "    for token in s.tokens:\n",
    "        if token in feature_list:\n",
    "            idx = feature_list.index(token)\n",
    "            count_list[idx] += 1\n",
    "    row = pd.DataFrame([count_list],columns =feature_list)\n",
    "    df = df.append(row,ignore_index=True)\n",
    "    row_target = pd.DataFrame([s.labeled_aspects],columns=['Prod_Feat'])\n",
    "    target = target.append(row_target,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_idx, test_idx = train_test_split(df.index, test_size=0.25, random_state=42)\n",
    "X_train  = df.iloc[train_idx]\n",
    "X_test = df.iloc[test_idx]\n",
    "y_train = target.iloc[train_idx]\n",
    "y_test = target.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree model construction, prediction, and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "model = tree.DecisionTreeClassifier(min_samples_leaf=5,max_depth=7)\n",
    "model = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "accuracy_score(y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_names = target.Prod_Feat.unique()\n",
    "print(classification_report(y_test, y_predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pydot \n",
    "from IPython.display import Image  \n",
    "from sklearn.externals.six import StringIO \n",
    "dot_data = StringIO()  \n",
    "# tree.export_graphviz(model, out_file=dot_data,  \n",
    "#                          feature_names=feature_list,  \n",
    "#                          class_names=target_names,  \n",
    "#                          filled=True, rounded=True,  \n",
    "#                          special_characters=True)  \n",
    "# graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "# Image(graph.create_png())\n",
    "with open(\"srs.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(model, out_file=f,  \n",
    "                         feature_names=feature_list,  \n",
    "                         class_names=target_names,  \n",
    "                         filled=True, rounded=True)  \n",
    "    # run on terminal \n",
    "    # $dot -Tpdf srs.dot -o srs.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Entropy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training using the training data (take some time, uncomment to proceed) or skip this and use pre-trained lambda instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# staticPredictor = StaticPredictor()\n",
    "# wordlist_filename = 'wordlist_dict_1.txt'\n",
    "# lamda_opt_filename = 'lambda_opt_75.txt'\n",
    "# training_set = []\n",
    "# for idx in train_idx:\n",
    "#     training_set.append(sentences[idx])\n",
    "# staticPredictor.train(wordlist_filename, lamda_opt_filename,training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#predict using trained \n",
    "staticPredictor = StaticPredictor()\n",
    "wordlist_filename = 'wordlist_dict_1.txt'\n",
    "# param_filename = 'lambda_opt_regu3.txt'\n",
    "param_filename = 'lambda_opt_75.txt'\n",
    "staticPredictor.loadParams(param_filename)\n",
    "staticPredictor.loadWordListDict(wordlist_filename)\n",
    "\n",
    "# example of prediction\n",
    "predicted_aspect = staticPredictor.predict(sentences[1])\n",
    "print predicted_aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test for accuracy using trained lambda\n",
    "correct = 0.0\n",
    "for idx in test_idx:\n",
    "    predicted_aspect = staticPredictor.predict(sentences[idx])\n",
    "    if predicted_aspect == sentences[idx].labeled_aspects:\n",
    "        correct +=1\n",
    "\n",
    "class_accuracy = correct/len(test_idx)\n",
    "print 'The classification accuracy is: %.2f' % (class_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA dimension reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X_train_mat = X_train.values\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train_mat)\n",
    "print(pca.explained_variance_ratio_) \n",
    "X_train_pca = pca.transform(X_train_mat)\n",
    "X_test_pca = pca.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit decision tree with PCA transformed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_pca = pd.DataFrame(X_train_pca)\n",
    "df_test_pca = pd.DataFrame(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_pca = tree.DecisionTreeClassifier(min_samples_leaf=5,max_depth=7)\n",
    "model_pca = model.fit(df_train_pca, y_train)\n",
    "y_predicted = model.predict(df_test_pca)\n",
    "accuracy_score(y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "mdl_rf = RandomForestClassifier(n_estimators=5,min_samples_leaf=5,max_depth=7)\n",
    "mdl_rf.fit(df_train_pca, y_train)\n",
    "y_predicted = mdl_rf.predict(df_test_pca)\n",
    "accuracy_score(y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "mdl_ada = AdaBoostClassifier(n_estimators=20,learning_rate=0.1)\n",
    "mdl_ada.fit(df_train_pca, y_train)\n",
    "y_predicted = mdl_ada.predict(df_test_pca)\n",
    "accuracy_score(y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mdl_nn = MLPClassifier(hidden_layer_sizes=10,activation='logistic',algorithm='sgd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "mdl_GNB = GaussianNB()\n",
    "mdl_GNB.fit(df_train_pca, y_train)\n",
    "y_predicted = mdl_GNB.predict(df_test_pca)\n",
    "accuracy_score(y_test,y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
